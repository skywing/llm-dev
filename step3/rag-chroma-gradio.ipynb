{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import LlamaCpp\n",
    "\n",
    "# If you following the instruction and use the q4_0 model, \n",
    "# you path should be: [UPDATE_AND_PUT_YOUR_PATH_TO_MODEL_HERE]/llama.cpp/models/llama-2-7b-chat/ggml-model-q4_0.gguf\n",
    "# the folloiwing code use the q5_0 model\n",
    "llm = LlamaCpp(\n",
    "    model_path=\"[PATH_TO_YOUR_MODEL]/llama.cpp/models/llama-2-7b-chat/ggml-model-q5_0.gguf\",\n",
    "    temperature=0.25,\n",
    "    n_gpu_layers=2,\n",
    "    n_batch=512,\n",
    "    n_ctx=2048,\n",
    "    f16_kv=True,\n",
    "    verbose=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import LlamaCppEmbeddings\n",
    "\n",
    "embeddings = LlamaCppEmbeddings(\n",
    "    model_path=\"[PATH_TO_YOUR_MODEL]/llama.cpp/models/llama-2-7b-chat/ggml-model-q5_0.gguf\",\n",
    "    verbose=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.document_loaders import PDFPlumberLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "def init_vectorstore(file_path):\n",
    "    contents = None\n",
    "    if file_path.endswith('.pdf'):\n",
    "        contents = PDFPlumberLoader(file_path).load()\n",
    "    elif file_path.endswith('.txt'):\n",
    "        contents = TextLoader(file_path).load()\n",
    "    \n",
    "    text_spliter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1000,\n",
    "        chunk_overlap=100,\n",
    "        length_function=len,\n",
    "        add_start_index=True\n",
    "    )\n",
    "\n",
    "    documents = text_spliter.split_documents(contents)\n",
    "    chromadb = Chroma.from_documents(documents, embeddings)\n",
    "    return chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt_template = \"\"\"\n",
    "[INST]<<SYS>> You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.<</SYS>> \n",
    "\n",
    "{context} \n",
    "\n",
    "Question: {question} \n",
    "Answer: [/INST]\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(prompt_template, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "with gr.Blocks() as ui:\n",
    "    user_session = gr.State({})\n",
    "    # call back function to predict the response\n",
    "    def predict(message, history, user_session):\n",
    "        if user_session.get(\"qa_chain\") is None:\n",
    "            return \"Please upload a document in text or PDF format for this sample to work.\"\n",
    "        \n",
    "        qa_chain = user_session[\"qa_chain\"]\n",
    "        \n",
    "        if len(message) == 0:\n",
    "            return \"Please ask a question related to the document you uploaded.\"\n",
    "\n",
    "        resp = qa_chain({\"query\": message})\n",
    "        return resp[\"result\"]\n",
    "    \n",
    "    # call back function to upload file for LLM context searching\n",
    "    def upload_file(file, user_session):\n",
    "        vectorstore = init_vectorstore(file.name)\n",
    "        qa_chain = RetrievalQA.from_chain_type(\n",
    "            llm, \n",
    "            retriever=vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\":5}),\n",
    "            chain_type_kwargs={\"prompt\": prompt},\n",
    "            verbose=True\n",
    "        )\n",
    "        user_session[\"qa_chain\"] = qa_chain\n",
    "        return  \"File splitted, embeded, and ready to be searched.\", user_session\n",
    "    \n",
    "    def clear_upload_file():\n",
    "        return \"\"\n",
    "    \n",
    "    with gr.Row():\n",
    "        chatui = None\n",
    "        with gr.Column():\n",
    "            chatui = gr.ChatInterface(\n",
    "                predict,\n",
    "                retry_btn=None,\n",
    "                undo_btn=None,\n",
    "                clear_btn=None,\n",
    "                submit_btn=\"Send\",\n",
    "                additional_inputs=[user_session])\n",
    "        with gr.Column():\n",
    "            ctx_text_box = gr.Textbox(lines=8, label=\"Documents\", placeholder=\"Only Text or PDF files is supported\")\n",
    "            file = gr.File(file_types=[\"txt\", \"pdf\"], label=\"Use the click to upload instead of drag and drop. Drag and drop doesn't work here.\")\n",
    "            file.upload(upload_file, inputs=[file, user_session], outputs=[ctx_text_box, user_session], show_progress=True)\n",
    "            file.clear(clear_upload_file, outputs=[ctx_text_box])\n",
    "\n",
    "ui.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ui.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
